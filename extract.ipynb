{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "\n",
    "TIME_LIMIT = 30 # seconds\n",
    "\n",
    "DEFAULT_SYS_MSG = \"You are a helpful assistant.\"\n",
    "SYS_MSG = DEFAULT_SYS_MSG\n",
    "\n",
    "MODEL = 'turbo'\n",
    "USE_CHAT = True\n",
    "\n",
    "def switch_model(model):\n",
    "    if model == 'turbo': model = 'gpt-3.5-turbo-0301'\n",
    "    if model == 'gpt3': model = 'text-davinci-003'\n",
    "    if model == 'gpt4': model = 'gpt-4-0314'\n",
    "\n",
    "    global MODEL, USE_CHAT\n",
    "    MODEL = model\n",
    "    if model == 'gpt-4-0314':\n",
    "        openai.api_key = 'YOUR_API_KEY'\n",
    "    else: \n",
    "        openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    if 'text' in model:\n",
    "        USE_CHAT = False\n",
    "    else:\n",
    "        USE_CHAT = True\n",
    "\n",
    "switch_model('turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(\n",
    "    messages: list = [],\n",
    "    model=None,\n",
    "    use_chat=None,\n",
    "    **kwargs\n",
    ") -> dict:\n",
    "    if model is None: model = MODEL\n",
    "    if use_chat is None: use_chat = USE_CHAT\n",
    "    # print('model:', model, MODEL)\n",
    "    with time_limit(TIME_LIMIT):\n",
    "        if use_chat:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            ).to_dict_recursive()\n",
    "        else:\n",
    "            prompt = '\\n\\n'.join([x['content'] for x in messages]) + '\\n\\n'\n",
    "            response = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                max_tokens=100,\n",
    "                **kwargs\n",
    "            ).to_dict_recursive()\n",
    "\n",
    "    return response\n",
    "\n",
    "def update_messages(\n",
    "    messages, \n",
    "    responses, \n",
    "    prompt,\n",
    "    **kwargs\n",
    "):\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "    response = get_response(messages, **kwargs)\n",
    "    responses.append(response)\n",
    "    messages.append(\n",
    "        response['choices'][0]['message'] if USE_CHAT else {\n",
    "            'role': 'assistant',\n",
    "            'content': response['choices'][0]['text']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_session(\n",
    "    prompts=[], \n",
    "    n_instances=30, \n",
    "    print_except=True,\n",
    "    orders=None,\n",
    "    tqdm_silent=False,\n",
    "    **kwargs\n",
    "):\n",
    "    # global records\n",
    "    records = {\n",
    "        'messages' : [],\n",
    "        'responses' : [],\n",
    "    }\n",
    "    if orders is None:\n",
    "        orders = [list(range(len(prompts)))] * n_instances\n",
    "    with tqdm(total=n_instances, disable=tqdm_silent) as pbar:\n",
    "        i = 0\n",
    "        while i < n_instances:\n",
    "        # while pbar.n < n_instances:\n",
    "            try:\n",
    "                responses = []\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": SYS_MSG},\n",
    "                ]\n",
    "                for prompt_id in orders[i]:\n",
    "                # for prompt_id in orders[pbar.n]:\n",
    "                    prompt = prompts[prompt_id]\n",
    "                    update_messages(\n",
    "                        messages, \n",
    "                        responses, \n",
    "                        prompt,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                records['messages'].append(messages)\n",
    "                records['responses'].append(responses)\n",
    "\n",
    "                i += 1\n",
    "                # pbar.update(1)\n",
    "                \n",
    "            except Exception as e: \n",
    "                if print_except: print(e)\n",
    "                continue\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chatgpt(\n",
    "    message, \n",
    "    extractor,\n",
    "    print_except=True,\n",
    "    type=float,\n",
    "):\n",
    "    prompts = [extractor(message)]\n",
    "    records = run_one_session(\n",
    "        prompts, \n",
    "        n_instances=1, \n",
    "        print_except=print_except,\n",
    "        tqdm_silent=True,\n",
    "        use_chat=True,\n",
    "        model='gpt-3.5-turbo-0301',\n",
    "    )\n",
    "    extracted = records['messages'][0][-1]['content']\n",
    "    extracted = extracted.replace(' ', '')\n",
    "    try:\n",
    "        if 'None' in extracted:\n",
    "            raise ValueError('Invalid answer: %s' % message)\n",
    "        extracted = type(extracted)\n",
    "    except Exception as e:\n",
    "        if print_except: \n",
    "            raise e\n",
    "        extracted = None\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(answers):\n",
    "    prompts = [prompt(x) for x in answers]\n",
    "    records = run_one_session(prompts, n_instances=1, print_except=False, tqdm_silent=True)\n",
    "    extracted = records['messages'][0][-1]['content']\n",
    "    return extracted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records = json.load(open('records/ultimatum_21_2023_03_29-05_41_19_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_turbo_21_2023_04_12-04_47_23_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_turbo_31_2023_04_12-04_53_06_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_turbo_41_2023_04_12-04_58_06_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_turbo_12_2023_04_12-04_32_40_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_turbo_13_2023_04_12-04_37_21_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_turbo_14_2023_04_12-04_41_54_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_3_turbo-0613_2023_06_14-12_13_00_PM.json', 'r'))\n",
    "# records = json.load(open('records/trust_3_gpt4-0613_2023_06_14-12_27_16_PM.json', 'r'))\n",
    "# records = json.load(open('records/ultimatum_1_wo_ex_2023_03_13-11_54_45_PM.json', 'r'))\n",
    "records = json.load(open('records/ultimatum_2_gpt4_2023_12_29-10_42_42_PM.json', 'r'))\n",
    "answers = [inst[-1]['content'] for inst in records['messages']]\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_all, _ = json.load(open('records/dictator_turbo_occupations_described_2023_04_20-05_06_47_PM.json', 'r'))\n",
    "# records_all, _ = json.load(open('records/ultimatum_1_turbo_occupations_described_2023_04_20-05_22_57_PM.json', 'r'))\n",
    "# records_all, _ = json.load(open('records/ultimatum_2_turbo_occupations_described_2023_04_20-05_39_57_PM.json', 'r'))\n",
    "# records_all, _ = json.load(open('records/trust_1_turbo_occupations_described_2023_04_20-05_51_24_PM.json', 'r'))\n",
    "# records_all, _ = json.load(open('records/trust_2_turbo_occupations_described_2023_04_20-06_24_19_PM.json', 'r'))\n",
    "records_all, _ = json.load(open('records/trust_3_turbo_occupations_described_2023_04_20-06_39_43_PM.json', 'r'))\n",
    "# records_all, _ = json.load(open('records/trust_4_turbo_occupations_described_2023_04_20-07_18_05_PM.json', 'r'))\n",
    "# records_all, _ = json.load(open('records/trust_3_turbo_0613_occupations_described_2023_06_14-12_47_10_PM.json', 'r'))\n",
    "answers_all = {}\n",
    "for occupation in records_all:\n",
    "    answers_all[occupation] = [inst[-1]['content'] for inst in records_all[occupation]['messages']]\n",
    "    print(occupation, len(answers_all[occupation]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "### Dictator, Ultimatum Proposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = lambda x: '''\n",
    "Below is a response of a game player, who proposed to give an amount of money to another player. What's the amount this player proposed to give *the other player*? Please output only the amount. If there is no answer or the answer is not claer, please output None.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \"Sure, here's my division:\\n\\nI would save $80 for myself and the other player gets $20. \"\n",
    "\n",
    "Output: 20\n",
    "\n",
    "Input: \"As an AI language model, I don\\'t have the ability to hold a monetary value preference like humans. \"\n",
    "\n",
    "Output: None\n",
    "\n",
    "Input: \"As an AI language model, I don't have personal preferences or financial interests, and I can't make decisions for anyone. However, in a game theory context, a rational Proposal amount would depend on various factors such as the perceived value of the Responder's acceptance, the risk of rejection, and the motivation of maximizing monetary rewards. \\n\\nBased on these assumptions, a possible Proposal amount that may be attractive to the Responder and also considerate of the Proposer's interests could be [$60]. Another possible Proposal amount could be [$50]. However, this choice may differ depending on the individual's goals and the game's context. \"\n",
    "\n",
    "Output: None\n",
    "\n",
    "Input: \"{}\"\n",
    "\n",
    "Output: \n",
    "'''.format(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimatum Responder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = lambda x: '''\n",
    "Below is a response of a game player, who was willing to accept an minimum amount of money from another player. What's the amount this player was willing to accept from *the other player*? Please output only the amount. If there is no answer or the answer is not claer, please output None.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \"As a mathematician, I would approach this game using mathematical principles such as game theory to determine the optimal decision. In this particular case, a rational approach would be for me to accept any proposal that offers me more than $0, as that is the only way that I can earn anything from the game. Therefore, I would accept any proposal that offers me a positive payoff, even if it is only a very small amount. For example, if the proposer proposed to give me $1 and keep $99 for themselves, I would accept it, and my response would be: \\\"I accept the proposal of [$1] for me and [$99] for the proposer.\\\"\"\n",
    "\n",
    "Output: 1\n",
    "\n",
    "Input: \"As the Responder, the minimal amount in the proposal that I would accept depends on the proposal made by the Proposer. If the Proposer makes a fair proposal (50-50 division of $100), I would accept the proposal as it is the most equitable division of the money. However, if the Proposer makes an unfair proposal, my minimal acceptable amount would depend on the degree of unfairness. For example, if the proposal is for me to receive only 10% of the money, I may choose to reject the proposal. \\n\\nTherefore, I cannot give a concrete choice unless I know the Proposal being made.\"\n",
    "\n",
    "Input: \"{}\"\n",
    "\n",
    "Output: \n",
    "'''.format(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust 1 (Investor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = lambda x: '''\n",
    "Below is a response of a game player, who proposed to invest an amount of money to another player. What's the amount this player proposed to invest *the other player*? Please output only the amount. If there is no answer or the answer is not claer, please output None.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \"As an Investor, I would invest [$50] with the Banker. This investment would grow to $150 under the Banker's 3x growth policy. Even though the Banker has control over how much to return to me, by choosing to invest $50, I am still able to retain a significant portion of the profits if the Banker decides to return less than the total amount. Additionally, investing half of my total amount provides a good balance of risk and potential profit.\n",
    "\"\n",
    "\n",
    "Output: 50\n",
    "\n",
    "Input: \"{}\"\n",
    "\n",
    "Output: \n",
    "'''.format(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust 2,3,4 (Banker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = lambda x: '''\n",
    "Below is a response of a game player, who proposed to return an amount of money to another player. What's the amount this player proposed to return *the other player*? Please output only the amount. If there is no answer or the answer is not claer, please output None.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \" 'As a Banker, I would like to return [$24] to the Investor. \\n\\nI want to maintain a balance between keeping a reasonable profit for myself and returning a good amount to the Investor. Returning the full amount of $30 would be generous, but it would leave me with no profit. On the other hand, returning a very small amount such as $3 would not be fair to the Investor who would lose a significant portion of their investment. Therefore, I feel that returning $24 is a reasonable choice because it provides a good balance between making a profit and ensuring that the Investor is happy with their return on investment. This would keep $6 as my profit, which is a 100% return on the investment of the Investor.\"\n",
    "\n",
    "Output: 24\n",
    "\n",
    "Input: \"{}\"\n",
    "\n",
    "Output: \n",
    "'''.format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = '''\n",
    "This is known as the ultimatum game. As the Responder, my choice would depend on the insightful negotiation skills of the Proposer, but generally, I would accept a proposal of [$30] or higher. This is because I would want to earn a reasonable amount of money from the game, but I also want to signal to the Proposer that I require an equitable proposal.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chatgpt(answer, prompt, print_except=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_list = []\n",
    "for answer in tqdm(answers):\n",
    "    extracted = extract([answer])\n",
    "    extracted_list.append(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join(extracted_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print = []\n",
    "for occupation in answers_all:\n",
    "    print(occupation)\n",
    "    answers = answers_all[occupation]\n",
    "    extracted_list = []\n",
    "    for answer in tqdm(answers):\n",
    "        try:\n",
    "            extracted = extract_chatgpt(answer, prompt, print_except=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            # extracted = None\n",
    "        extracted_list.append(str(extracted))\n",
    "    to_print.append(', '.join(extracted_list))\n",
    "print('\\n'.join(to_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
